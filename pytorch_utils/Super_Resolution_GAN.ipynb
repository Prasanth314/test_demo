{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Super Resolution GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_ShavWAglYG",
        "outputId": "64ef135f-1786-4346-a59c-074fed6c1710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'a-PyTorch-Tutorial-to-Super-Resolution'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Total 47 (delta 0), reused 0 (delta 0), pack-reused 47\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Lornatang/CycleGAN-PyTorch"
      ],
      "metadata": {
        "id": "KDZMgT_Vg53s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21083cae-d0df-43c6-a5cc-01e7b9b81c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CycleGAN-PyTorch'...\n",
            "remote: Enumerating objects: 440, done.\u001b[K\n",
            "remote: Counting objects: 100% (437/437), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 440 (delta 245), reused 426 (delta 238), pack-reused 3\u001b[K\n",
            "Receiving objects: 100% (440/440), 1.38 MiB | 30.13 MiB/s, done.\n",
            "Resolving deltas: 100% (245/245), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jayeshsaita/Day-Night-Classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdPO7_R5u5JY",
        "outputId": "fc764dba-d280-4295-96f1-2b1b2b166251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Day-Night-Classifier'...\n",
            "remote: Enumerating objects: 1303, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 1303 (delta 8), reused 27 (delta 5), pack-reused 1271\u001b[K\n",
            "Receiving objects: 100% (1303/1303), 91.01 MiB | 33.18 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/CycleGAN-PyTorch/data/Day2night\n",
        "!mkdir /content/CycleGAN-PyTorch/data/Day2night/train\n",
        "!mkdir /content/CycleGAN-PyTorch/data/Day2night/train/A\n",
        "!mkdir /content/CycleGAN-PyTorch/data/Day2night/train/B"
      ],
      "metadata": {
        "id": "hcmdzUVbrEL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/Day-Night-Classifier/day_night_dataset/train/day/* /content/CycleGAN-PyTorch/data/Day2night/train/A\n",
        "!cp /content/Day-Night-Classifier/day_night_dataset/val/day/* /content/CycleGAN-PyTorch/data/Day2night/train/A\n",
        "!cp /content/Day-Night-Classifier/day_night_dataset/train/night/* /content/CycleGAN-PyTorch/data/Day2night/train/B\n",
        "!cp /content/Day-Night-Classifier/day_night_dataset/val/night/* /content/CycleGAN-PyTorch/data/Day2night/train/B"
      ],
      "metadata": {
        "id": "oIhjJxQDrNux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/a-PyTorch-Tutorial-to-Super-Resolution/models.py /content/CycleGAN-PyTorch/\n",
        "!cp /content/a-PyTorch-Tutorial-to-Super-Resolution/utils.py /content/CycleGAN-PyTorch/"
      ],
      "metadata": {
        "id": "IA3FmQ-p7fiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/CycleGAN-PyTorch/train.py --dataroot /content/CycleGAN-PyTorch/data --dataset Day2night --cuda"
      ],
      "metadata": {
        "id": "CwXxy-gsu9JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-NaaH6xCdH42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://drive.google.com/file/d/1EfNry1L-kX5cib7VO70mjHA7pYmtX995/"
      ],
      "metadata": {
        "id": "zSxMWCKhT-0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/a-PyTorch-Tutorial-to-Super-Resolution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErCmc3Ovlt9i",
        "outputId": "1cdaa5f9-310b-417f-bec2-2d2f5aa99f44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/a-PyTorch-Tutorial-to-Super-Resolution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import itertools\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "srtransform = transforms.Compose([\n",
        "                           transforms.Resize(int(256), Image.BICUBIC),\n",
        "                           transforms.RandomCrop(256),\n",
        "                          #  transforms.RandomHorizontalFlip(),\n",
        "                           transforms.ToTensor()])\n",
        "\n",
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch\n",
        "from torch import nn\n",
        "from models import SRResNet\n",
        "from datasets import SRDataset\n",
        "from utils import *\n",
        "import torch.nn.functional as F\n",
        "# Data parameters\n",
        "data_folder = './'  # folder with JSON data files\n",
        "crop_size = 96  # crop size of target HR images\n",
        "scaling_factor = 4  # the scaling factor for the generator; the input LR images will be downsampled from the target HR images by this factor\n",
        "\n",
        "# Model parameters\n",
        "large_kernel_size = 9  # kernel size of the first and last convolutions which transform the inputs and outputs\n",
        "small_kernel_size = 3  # kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n",
        "n_channels = 64  # number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n",
        "n_blocks = 16  # number of residual blocks\n",
        "\n",
        "# Learning parameters\n",
        "checkpoint = '/content/checkpoint_srresnet.pth.tar'  # path to model checkpoint, None if none\n",
        "batch_size = 16  # batch size\n",
        "start_epoch = 0  # start at this epoch\n",
        "iterations = 1e6  # number of training iterations\n",
        "workers = 4  # number of workers for loading data in the DataLoader\n",
        "print_freq = 500  # print training status once every __ batches\n",
        "lr = 1e-4  # learning rate\n",
        "grad_clip = None  # clip if gradients are exploding\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "cudnn.benchmark = True\n",
        "checkpoint = torch.load(checkpoint)\n",
        "\n",
        "model = checkpoint['model']\n",
        "optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                                     lr=lr)\n",
        "img = Image.open('/content/baboon.png', mode='r')\n",
        "img = img.convert('RGB')\n",
        "img =srtransform(img)\n",
        "img = img.to(device)\n",
        "imagenet_mean = torch.FloatTensor([0.485, 0.456, 0.406]).unsqueeze(1).unsqueeze(2)\n",
        "imagenet_std = torch.FloatTensor([0.229, 0.224, 0.225]).unsqueeze(1).unsqueeze(2)\n",
        "imagenet_mean_cuda = torch.FloatTensor([0.485, 0.456, 0.406]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
        "imagenet_std_cuda = torch.FloatTensor([0.229, 0.224, 0.225]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
        "img = img.unsqueeze(0)\n",
        "print(\"min:\",torch.min(img))\n",
        "print(\"max:\",torch.max(img))\n",
        "if img.ndimension() == 3:\n",
        "    img = (img - imagenet_mean) / imagenet_std\n",
        "elif img.ndimension() == 4:\n",
        "    img = (img - imagenet_mean_cuda) / imagenet_std_cuda\n",
        "\n",
        "\n",
        "\n",
        "sr_imgs = model(img)\n",
        "sr_imgs = (sr_imgs+1)/2\n",
        "sr_imgs = F.interpolate(sr_imgs,size=512,mode='bilinear')\n",
        "vutils.save_image(sr_imgs,f\"/content/SR_translated.png\", normalize=True)"
      ],
      "metadata": {
        "id": "OpG45gw5oPeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e92ee0-0916-4336-86c1-bc90927fe57d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.activation.PReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.pixelshuffle.PixelShuffle' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Tanh' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min: tensor(0., device='cuda:0')\n",
            "max: tensor(0.9804, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn((1, 3, 5,5))\n",
        "print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiNuvoXKXq9x",
        "outputId": "e590aaa0-a12c-4ccd-c4da-674d62879021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 1.7043,  0.1272,  2.2886,  0.5185,  0.5069],\n",
            "          [-1.6438,  0.2764,  0.1816, -1.6745,  1.1881],\n",
            "          [-0.4356,  0.0664, -1.6435, -0.9383, -2.2518],\n",
            "          [-1.5022,  1.6561, -2.0535,  0.2920,  0.5376],\n",
            "          [ 0.5800, -2.3558,  0.4539, -0.8913, -0.2394]],\n",
            "\n",
            "         [[ 0.3562, -1.1237, -2.8899, -1.5435,  2.2437],\n",
            "          [ 1.3330,  0.7521, -0.3104,  1.9953, -0.2463],\n",
            "          [ 0.9327,  2.3944, -0.1064,  0.3872,  1.7508],\n",
            "          [-0.7213,  0.2644,  1.2730,  0.1761, -0.9506],\n",
            "          [-0.8950,  1.2898, -0.0570,  2.4474, -0.6436]],\n",
            "\n",
            "         [[ 0.8123, -0.0111,  0.6140, -1.1497, -0.4176],\n",
            "          [-1.6800, -1.1179, -0.0838, -1.1088,  0.2070],\n",
            "          [ 0.2427, -0.6679,  0.8796,  1.6698, -1.2682],\n",
            "          [-0.7439, -1.3174,  0.5307, -0.7725,  1.5377],\n",
            "          [-1.3022, -2.4867, -0.1349, -0.8690,  0.3605]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,[2,1,0],:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8PK_BDRXro9",
        "outputId": "a3f2b3d4-c854-4085-814d-b5f8cefb0e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.8123, -0.0111,  0.6140, -1.1497, -0.4176],\n",
              "          [-1.6800, -1.1179, -0.0838, -1.1088,  0.2070],\n",
              "          [ 0.2427, -0.6679,  0.8796,  1.6698, -1.2682],\n",
              "          [-0.7439, -1.3174,  0.5307, -0.7725,  1.5377],\n",
              "          [-1.3022, -2.4867, -0.1349, -0.8690,  0.3605]],\n",
              "\n",
              "         [[ 0.3562, -1.1237, -2.8899, -1.5435,  2.2437],\n",
              "          [ 1.3330,  0.7521, -0.3104,  1.9953, -0.2463],\n",
              "          [ 0.9327,  2.3944, -0.1064,  0.3872,  1.7508],\n",
              "          [-0.7213,  0.2644,  1.2730,  0.1761, -0.9506],\n",
              "          [-0.8950,  1.2898, -0.0570,  2.4474, -0.6436]],\n",
              "\n",
              "         [[ 1.7043,  0.1272,  2.2886,  0.5185,  0.5069],\n",
              "          [-1.6438,  0.2764,  0.1816, -1.6745,  1.1881],\n",
              "          [-0.4356,  0.0664, -1.6435, -0.9383, -2.2518],\n",
              "          [-1.5022,  1.6561, -2.0535,  0.2920,  0.5376],\n",
              "          [ 0.5800, -2.3558,  0.4539, -0.8913, -0.2394]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x+"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "_GkfEqRuX6Jw",
        "outputId": "4494b13b-7a1d-4dde-a743-fee80e8b0b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d42d9f5878e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: transpose() received an invalid combination of arguments - got (tuple), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1e0bAo_EX7Kr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}